{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d6094c",
   "metadata": {},
   "source": [
    "# GPyTorch\n",
    "#### Supposed to be great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6528dcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpytorch\n",
      "  Downloading gpytorch-1.10-py3-none-any.whl (255 kB)\n",
      "     -------------------------------------- 255.2/255.2 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from gpytorch) (1.0.2)\n",
      "Collecting linear-operator>=0.4.0\n",
      "  Downloading linear_operator-0.4.0-py3-none-any.whl (156 kB)\n",
      "     -------------------------------------- 156.7/156.7 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting torch>=1.11\n",
      "  Downloading torch-2.0.0-cp39-cp39-win_amd64.whl (172.3 MB)\n",
      "     ------------------------------------- 172.3/172.3 MB 12.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from linear-operator>=0.4.0->gpytorch) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from scikit-learn->gpytorch) (1.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from scikit-learn->gpytorch) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from scikit-learn->gpytorch) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\eminu\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11->linear-operator>=0.4.0->gpytorch) (1.2.1)\n",
      "Installing collected packages: torch, linear-operator, gpytorch\n",
      "Successfully installed gpytorch-1.10 linear-operator-0.4.0 torch-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gpytorch #you need to have torch installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "811c17f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from math import floor\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c596b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/eminu/OneDrive/Desktop/Air-Quality-StatsLab/Data/Rawdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3c4b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_cleaned_final.csv\")\n",
    "df_subsampled = df.sample(frac = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d654326",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df_subsampled[\"device\"], prefix = \"device\")\n",
    "df_subsampled = pd.concat([df_subsampled, dummy_df], axis = 1)\n",
    "df_subsampled = df_subsampled.drop(\"device\", axis = 1)\n",
    "\n",
    "X_large = pd.concat([df_subsampled.filter(regex=\"device_*\"), df_subsampled[[\"lat\", \"lon\", \"industrial\", \"major.road\",\n",
    "                  \"res.road\"]]], axis = 1).to_numpy()\n",
    "X = df_subsampled[[\"lat\",\"lon\"]].to_numpy()\n",
    "X = torch.from_numpy(X).double()\n",
    "\n",
    "Y = df_subsampled[[\"pm25_detrended_15\"]].to_numpy()\n",
    "Y = torch.from_numpy(Y).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92fd2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = Y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = Y[train_n:].contiguous()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4bfe6c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619, 1619)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.size(0), train_x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3c434229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inducing_points = inducing_points.cuda()\n",
    "\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=d)\n",
    "\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "    \n",
    "k = 32\n",
    "training_batch_size = 32\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPModel(inducing_points=train_x, likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    likelihood = likelihood.cuda()\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d6404ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = TensorDataset(train_x, train_y)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "#test_dataset = TensorDataset(test_x, test_y)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b80c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335f43ec0c86490e828b0acfca6819c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abc7a741b4c44128c860f8bca1c9586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 701 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7584\\1930716160.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Obtain the y_batch using indices. It is important to keep the same order of train_x and train_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_training_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 701 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "num_batches = model.variational_strategy._total_training_batches\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Our loss object. We're using the Variational ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for epoch in epochs_iter:\n",
    "    minibatch_iter = tqdm(range(num_batches), desc=\"Minibatch\", leave=False)\n",
    "\n",
    "    for i in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x=None)\n",
    "        \n",
    "        # Obtain the indices for mini-batch data\n",
    "        current_training_indices = model.variational_strategy.current_training_indices\n",
    "        \n",
    "        # Obtain the y_batch using indices. It is important to keep the same order of train_x and train_y\n",
    "        y_batch = train_y[...,current_training_indices]\n",
    "        if torch.cuda.is_available():\n",
    "            y_batch = y_batch.cuda()\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "168eccdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bee9c1e885f414b9e204d63746e8906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683a55971f3e40a9a929ad60d9437898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 128 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7584\\357739745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mminibatch_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 128 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "# prepare for dataset\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "# this batch-size does not need to match the training-batch-size specified above\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "test_mse = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "\n",
    "        diff = torch.pow(preds.mean - y_batch, 2)\n",
    "        diff = diff.sum(dim=-1) / test_x.size(0) # sum over bsz and scaling\n",
    "        diff = diff.mean() # average over likelihood_nsamples\n",
    "        test_mse += diff\n",
    "means = means[1:]\n",
    "test_rmse = test_mse.sqrt().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
